# YMS Dashboard Stress Testing Framework Implementation

## 1. Project Structure

```
yms-stress-testing-framework/
├── jmeter/
│   ├── test-plans/
│   │   ├── YMS-Dashboard-Master-Test.jmx
│   │   ├── templates/
│   │   │   ├── endpoint-template.jmx
│   │   │   └── tenant-template.jmx
│   │   └── scenarios/
│   │       ├── smoke-test.jmx
│   │       ├── load-test.jmx
│   │       └── stress-test.jmx
│   ├── data/
│   │   ├── tenant-data.csv
│   │   └── test-data/
│   └── lib/
│       └── jython-standalone-2.7.3.jar
├── python/
│   ├── setup.py
│   ├── requirements.txt
│   ├── config/
│   │   ├── tenant_config.yaml
│   │   ├── endpoint_config.yaml
│   │   └── test_scenarios.yaml
│   ├── generators/
│   │   ├── __init__.py
│   │   ├── base_generator.py
│   │   ├── payload_generators.py
│   │   └── data_pool.py
│   ├── validators/
│   │   ├── __init__.py
│   │   ├── base_validator.py
│   │   └── response_validators.py
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── tenant_manager.py
│   │   ├── metrics_collector.py
│   │   ├── logger.py
│   │   └── report_generator.py
│   └── cli.py
├── logs/
│   ├── jmeter/
│   └── python/
├── reports/
│   ├── html/
│   ├── csv/
│   └── summary/
├── scripts/
│   ├── run-local.sh
│   ├── run-distributed.sh
│   ├── generate-reports.sh
│   └── analyze-results.py
└── docs/
    ├── setup-guide.md
    ├── execution-guide.md
    └── troubleshooting.md
```

## 2. Python Implementation

### 2.1 Base Generator (base_generator.py)

```python
import json
import random
from abc import ABC, abstractmethod
from typing import Dict, Any, List
import yaml
from datetime import datetime, timedelta

class BaseGenerator(ABC):
    """Base class for all payload generators"""
    
    def __init__(self, tenant_config: Dict[str, Any]):
        self.tenant_config = tenant_config
        self.tenant_name = tenant_config.get('name')
        self.data_pool = tenant_config.get('data_pool', {})
        
    @abstractmethod
    def generate(self) -> Dict[str, Any]:
        """Generate payload specific to the endpoint"""
        pass
    
    def get_random_facility_id(self) -> int:
        """Get random facility ID from tenant's pool"""
        facility_ids = self.data_pool.get('facility_ids', [1001, 1002, 1003])
        return random.choice(facility_ids)
    
    def get_random_carrier_ids(self, count: int = None) -> List[int]:
        """Get random carrier IDs from tenant's pool"""
        carrier_ids = self.data_pool.get('carrier_ids', [2001, 2002, 2003, 2004])
        if count is None:
            count = random.randint(1, len(carrier_ids))
        return random.sample(carrier_ids, min(count, len(carrier_ids)))
    
    def to_json(self) -> str:
        """Convert generated payload to JSON string"""
        return json.dumps(self.generate())
```

### 2.2 Payload Generators (payload_generators.py)

```python
from .base_generator import BaseGenerator
from typing import Dict, Any
import random

class FacilityRequestGenerator(BaseGenerator):
    """Generator for FacilityRequestPayloadDTO"""
    
    def generate(self) -> Dict[str, Any]:
        return {
            "facilityId": self.get_random_facility_id()
        }

class TrailerOverviewRequestGenerator(BaseGenerator):
    """Generator for TrailerOverviewRequestPayloadDTO"""
    
    TRAILER_STATES = ["all", "noFlags", "audit", "damaged", "outOfService"]
    
    def generate(self) -> Dict[str, Any]:
        return {
            "facilityId": self.get_random_facility_id(),
            "carrierIds": self.get_random_carrier_ids(),
            "trailerState": random.choice(self.TRAILER_STATES)
        }

class TrailerExceptionsRequestGenerator(BaseGenerator):
    """Generator for TrailerExceptionsRequestPayloadDTO"""
    
    def generate(self) -> Dict[str, Any]:
        return {
            "facilityId": self.get_random_facility_id(),
            "carrierIds": self.get_random_carrier_ids(),
            "lastDetectionTimeThresholdHours": random.randint(1, 24),
            "inboundLoadedThresholdHours": random.randint(4, 48)
        }

class FacilityCarrierRequestGenerator(BaseGenerator):
    """Generator for FacilityCarrierRequestPayloadDTO"""
    
    def generate(self) -> Dict[str, Any]:
        return {
            "facilityId": self.get_random_facility_id(),
            "carrierIds": self.get_random_carrier_ids()
        }

class ShipmentVolumeForecastRequestGenerator(BaseGenerator):
    """Generator for ShipmentVolumeForecastRequestPayloadDTO"""
    
    SHIPMENT_DIRECTIONS = ["Inbound", "Outbound"]
    
    def generate(self) -> Dict[str, Any]:
        return {
            "facilityId": self.get_random_facility_id(),
            "carrierIds": self.get_random_carrier_ids(),
            "shipmentDirection": random.choice(self.SHIPMENT_DIRECTIONS)
        }

class SavedFilterTrailerCountRequestGenerator(BaseGenerator):
    """Generator for SavedFilterTrailerCountRequestPayloadDTO"""
    
    def generate(self) -> Dict[str, Any]:
        user_ids = self.data_pool.get('user_ids', [3001, 3002, 3003])
        saved_filter_ids = self.data_pool.get('saved_filter_ids', [4001, 4002, 4003])
        
        return {
            "facilityId": self.get_random_facility_id(),
            "savedFilterId": random.choice(saved_filter_ids),
            "userId": random.choice(user_ids)
        }

# Generator factory
GENERATORS = {
    "yard-availability": FacilityRequestGenerator,
    "trailer-overview": TrailerOverviewRequestGenerator,
    "trailer-exception-summary": TrailerExceptionsRequestGenerator,
    "task-workload-summary": FacilityRequestGenerator,
    "task-attention-summary": FacilityRequestGenerator,
    "site-occupancy": FacilityCarrierRequestGenerator,
    "shipment-volume-forecast": ShipmentVolumeForecastRequestGenerator,
    "dwell-time-summary": FacilityCarrierRequestGenerator,
    "door-breakdown-summary": FacilityRequestGenerator,
    "detention-summary": FacilityCarrierRequestGenerator,
    "saved-filter-trailer-count": SavedFilterTrailerCountRequestGenerator
}

def get_generator(endpoint: str, tenant_config: Dict[str, Any]) -> BaseGenerator:
    """Factory method to get appropriate generator"""
    generator_class = GENERATORS.get(endpoint)
    if not generator_class:
        raise ValueError(f"No generator found for endpoint: {endpoint}")
    return generator_class(tenant_config)
```

### 2.3 Response Validators (response_validators.py)

```python
import json
from typing import Dict, Any, List, Tuple
from datetime import datetime

class ResponseValidator:
    """Validates API responses"""
    
    def __init__(self):
        self.validation_results = []
    
    def validate_response(self, 
                         endpoint: str,
                         response_code: int,
                         response_body: str,
                         response_time: float) -> Tuple[bool, Dict[str, Any]]:
        """Validate complete response"""
        
        results = {
            "endpoint": endpoint,
            "timestamp": datetime.now().isoformat(),
            "response_code": response_code,
            "response_time": response_time,
            "validations": {}
        }
        
        # Status code validation
        status_valid = self.validate_status_code(response_code)
        results["validations"]["status_code"] = {
            "valid": status_valid,
            "expected": 200,
            "actual": response_code
        }
        
        # Response time validation (SLA: 2 seconds)
        time_valid = self.validate_response_time(response_time, sla=2000)
        results["validations"]["response_time"] = {
            "valid": time_valid,
            "sla_ms": 2000,
            "actual_ms": response_time
        }
        
        # Response body validation
        if response_body:
            body_valid, body_errors = self.validate_response_body(endpoint, response_body)
            results["validations"]["response_body"] = {
                "valid": body_valid,
                "errors": body_errors
            }
        else:
            body_valid = False
            results["validations"]["response_body"] = {
                "valid": False,
                "errors": ["Empty response body"]
            }
        
        # Overall validation
        overall_valid = status_valid and time_valid and body_valid
        results["overall_valid"] = overall_valid
        
        return overall_valid, results
    
    def validate_status_code(self, status_code: int) -> bool:
        """Validate HTTP status code"""
        return status_code == 200
    
    def validate_response_time(self, response_time: float, sla: float) -> bool:
        """Validate response time against SLA"""
        return response_time <= sla
    
    def validate_response_body(self, endpoint: str, response_body: str) -> Tuple[bool, List[str]]:
        """Validate response body structure"""
        errors = []
        
        try:
            data = json.loads(response_body)
        except json.JSONDecodeError:
            return False, ["Invalid JSON response"]
        
        # Endpoint-specific validations
        if endpoint == "yard-availability":
            errors.extend(self._validate_yard_availability(data))
        elif endpoint == "trailer-overview":
            errors.extend(self._validate_trailer_overview(data))
        # Add more endpoint-specific validations...
        
        return len(errors) == 0, errors
    
    def _validate_yard_availability(self, data: Dict[str, Any]) -> List[str]:
        """Validate yard availability response"""
        errors = []
        
        if "docks" not in data:
            errors.append("Missing 'docks' field")
        if "parkingSpots" not in data:
            errors.append("Missing 'parkingSpots' field")
        
        for field in ["docks", "parkingSpots"]:
            if field in data:
                errors.extend(self._validate_availability_metrics(data[field], field))
        
        return errors
    
    def _validate_trailer_overview(self, data: Dict[str, Any]) -> List[str]:
        """Validate trailer overview response"""
        errors = []
        
        if "totalTrailersCount" not in data:
            errors.append("Missing 'totalTrailersCount' field")
        elif not isinstance(data["totalTrailersCount"], int) or data["totalTrailersCount"] < 0:
            errors.append("Invalid 'totalTrailersCount' value")
        
        if "trailerOverviewMetrics" not in data:
            errors.append("Missing 'trailerOverviewMetrics' field")
        else:
            metrics = data["trailerOverviewMetrics"]
            required_fields = ["loadedInbound", "loadedOutbound", "empty", "partial"]
            for field in required_fields:
                if field not in metrics:
                    errors.append(f"Missing '{field}' in trailerOverviewMetrics")
        
        return errors
    
    def _validate_availability_metrics(self, metrics: Dict[str, Any], metric_name: str) -> List[str]:
        """Validate availability metrics structure"""
        errors = []
        
        if "total" not in metrics:
            errors.append(f"Missing 'total' in {metric_name}")
        
        for status in ["active", "inactive"]:
            if status not in metrics:
                errors.append(f"Missing '{status}' in {metric_name}")
            elif not self._validate_percentage_breakdown(metrics[status]):
                errors.append(f"Invalid percentage breakdown for {status} in {metric_name}")
        
        return errors
    
    def _validate_percentage_breakdown(self, breakdown: Dict[str, Any]) -> bool:
        """Validate percentage breakdown structure"""
        if not isinstance(breakdown, dict):
            return False
        
        required_fields = ["count", "percentage"]
        for field in required_fields:
            if field not in breakdown:
                return False
            if not isinstance(breakdown[field], int) or breakdown[field] < 0:
                return False
        
        return breakdown["percentage"] <= 100
```

### 2.4 Tenant Manager (tenant_manager.py)

```python
import yaml
from typing import Dict, Any, List
import os

class TenantManager:
    """Manages tenant configurations and data"""
    
    def __init__(self, config_path: str = "config/tenant_config.yaml"):
        self.config_path = config_path
        self.tenants = self._load_tenant_config()
    
    def _load_tenant_config(self) -> Dict[str, Any]:
        """Load tenant configuration from YAML file"""
        with open(self.config_path, 'r') as f:
            config = yaml.safe_load(f)
        return config.get('tenants', {})
    
    def get_tenant_config(self, tenant_name: str) -> Dict[str, Any]:
        """Get configuration for specific tenant"""
        if tenant_name not in self.tenants:
            raise ValueError(f"Tenant '{tenant_name}' not found in configuration")
        return self.tenants[tenant_name]
    
    def get_all_tenants(self) -> List[str]:
        """Get list of all configured tenants"""
        return list(self.tenants.keys())
    
    def get_tenant_rpm_config(self, tenant_name: str) -> Dict[str, int]:
        """Get RPM configuration for tenant"""
        tenant_config = self.get_tenant_config(tenant_name)
        return tenant_config.get('rpm_config', {})
    
    def get_tenant_auth_token(self, tenant_name: str) -> str:
        """Get authentication token for tenant"""
        tenant_config = self.get_tenant_config(tenant_name)
        return tenant_config.get('auth', {}).get('token', '')
```

### 2.6 Logger Utility (logger.py)

```python
import logging
import os
from datetime import datetime
from typing import Optional

class StressTestLogger:
    """Centralized logging for stress testing framework"""
    
    def __init__(self, name: str, log_dir: str = "logs/python"):
        self.name = name
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        
        # Create logger
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.DEBUG)
        
        # File handler for all logs
        log_file = os.path.join(log_dir, f"{name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)
        
        # Console handler for important logs
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # Add handlers
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
    
    def info(self, message: str):
        self.logger.info(message)
    
    def error(self, message: str, exc_info: Optional[Exception] = None):
        self.logger.error(message, exc_info=exc_info)
    
    def warning(self, message: str):
        self.logger.warning(message)
    
    def debug(self, message: str):
        self.logger.debug(message)

# Global logger instance
stress_logger = StressTestLogger("stress-test")
```

### 2.7 Report Generator (report_generator.py)

```python
import csv
import json
import os
from datetime import datetime
from typing import Dict, List, Any
import statistics
from collections import defaultdict

class ReportGenerator:
    """Generate HTML and CSV reports from test results"""
    
    def __init__(self, results_dir: str = "reports"):
        self.results_dir = results_dir
        os.makedirs(results_dir, exist_ok=True)
        os.makedirs(os.path.join(results_dir, "html"), exist_ok=True)
        os.makedirs(os.path.join(results_dir, "csv"), exist_ok=True)
        os.makedirs(os.path.join(results_dir, "summary"), exist_ok=True)
    
    def generate_summary_report(self, jtl_file: str, tenant: str) -> Dict[str, Any]:
        """Generate summary statistics from JTL file"""
        
        results = {
            "tenant": tenant,
            "test_date": datetime.now().isoformat(),
            "total_requests": 0,
            "failed_requests": 0,
            "error_rate": 0.0,
            "endpoints": defaultdict(lambda: {
                "count": 0,
                "errors": 0,
                "response_times": [],
                "min_rt": 0,
                "max_rt": 0,
                "avg_rt": 0,
                "p90_rt": 0,
                "p95_rt": 0,
                "p99_rt": 0
            })
        }
        
        # Parse JTL file
        with open(jtl_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                endpoint = row.get('label', '')
                success = row.get('success', 'true') == 'true'
                response_time = int(row.get('elapsed', 0))
                
                results["total_requests"] += 1
                if not success:
                    results["failed_requests"] += 1
                
                # Endpoint specific stats
                ep_stats = results["endpoints"][endpoint]
                ep_stats["count"] += 1
                ep_stats["response_times"].append(response_time)
                if not success:
                    ep_stats["errors"] += 1
        
        # Calculate statistics
        results["error_rate"] = (results["failed_requests"] / results["total_requests"] * 100) if results["total_requests"] > 0 else 0
        
        for endpoint, stats in results["endpoints"].items():
            if stats["response_times"]:
                stats["min_rt"] = min(stats["response_times"])
                stats["max_rt"] = max(stats["response_times"])
                stats["avg_rt"] = statistics.mean(stats["response_times"])
                stats["p90_rt"] = statistics.quantiles(stats["response_times"], n=10)[8] if len(stats["response_times"]) > 10 else stats["max_rt"]
                stats["p95_rt"] = statistics.quantiles(stats["response_times"], n=20)[18] if len(stats["response_times"]) > 20 else stats["max_rt"]
                stats["p99_rt"] = statistics.quantiles(stats["response_times"], n=100)[98] if len(stats["response_times"]) > 100 else stats["max_rt"]
                stats["error_rate"] = (stats["errors"] / stats["count"] * 100) if stats["count"] > 0 else 0
                # Remove raw response times from final report
                del stats["response_times"]
        
        return results
    
    def generate_html_report(self, summary: Dict[str, Any], output_file: str):
        """Generate HTML report from summary data"""
        
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>YMS Dashboard Stress Test Report - {summary['tenant']}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        h1, h2 {{ color: #333; }}
        .summary {{ background: #f0f0f0; padding: 15px; border-radius: 5px; margin: 20px 0; }}
        .metric {{ display: inline-block; margin: 10px 20px 10px 0; }}
        .metric-label {{ font-weight: bold; }}
        .metric-value {{ font-size: 1.2em; }}
        .error {{ color: #d9534f; }}
        .success {{ color: #5cb85c; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #4CAF50; color: white; }}
        tr:nth-child(even) {{ background-color: #f2f2f2; }}
        .warning {{ background-color: #fff3cd; }}
        .danger {{ background-color: #f8d7da; }}
    </style>
</head>
<body>
    <h1>YMS Dashboard Stress Test Report</h1>
    
    <div class="summary">
        <h2>Test Summary</h2>
        <div class="metric">
            <span class="metric-label">Tenant:</span>
            <span class="metric-value">{summary['tenant']}</span>
        </div>
        <div class="metric">
            <span class="metric-label">Test Date:</span>
            <span class="metric-value">{datetime.fromisoformat(summary['test_date']).strftime('%Y-%m-%d %H:%M:%S')}</span>
        </div>
        <div class="metric">
            <span class="metric-label">Total Requests:</span>
            <span class="metric-value">{summary['total_requests']:,}</span>
        </div>
        <div class="metric">
            <span class="metric-label">Failed Requests:</span>
            <span class="metric-value {'' if summary['failed_requests'] == 0 else 'error'}">{summary['failed_requests']:,}</span>
        </div>
        <div class="metric">
            <span class="metric-label">Error Rate:</span>
            <span class="metric-value {'' if summary['error_rate'] < 1 else 'error'}">{summary['error_rate']:.2f}%</span>
        </div>
    </div>
    
    <h2>Endpoint Performance</h2>
    <table>
        <tr>
            <th>Endpoint</th>
            <th>Requests</th>
            <th>Errors</th>
            <th>Error Rate</th>
            <th>Min RT (ms)</th>
            <th>Avg RT (ms)</th>
            <th>P90 RT (ms)</th>
            <th>P95 RT (ms)</th>
            <th>P99 RT (ms)</th>
            <th>Max RT (ms)</th>
        </tr>
"""
        
        for endpoint, stats in summary['endpoints'].items():
            row_class = ''
            if stats['error_rate'] > 5:
                row_class = 'danger'
            elif stats['error_rate'] > 1:
                row_class = 'warning'
            
            html_content += f"""
        <tr class="{row_class}">
            <td>{endpoint}</td>
            <td>{stats['count']:,}</td>
            <td>{stats['errors']:,}</td>
            <td>{stats['error_rate']:.2f}%</td>
            <td>{stats['min_rt']:.0f}</td>
            <td>{stats['avg_rt']:.0f}</td>
            <td>{stats['p90_rt']:.0f}</td>
            <td>{stats['p95_rt']:.0f}</td>
            <td>{stats['p99_rt']:.0f}</td>
            <td>{stats['max_rt']:.0f}</td>
        </tr>
"""
        
        html_content += """
    </table>
    
    <div class="summary">
        <h3>SLA Compliance</h3>
        <p>✓ Response Time SLA (P95 < 2000ms): <span class="metric-value">""" + (
            "PASS" if all(stats['p95_rt'] < 2000 for stats in summary['endpoints'].values()) else "FAIL"
        ) + """</span></p>
        <p>✓ Error Rate SLA (< 1%): <span class="metric-value">""" + (
            "PASS" if summary['error_rate'] < 1 else "FAIL"
        ) + """</span></p>
    </div>
</body>
</html>
"""
        
        with open(output_file, 'w') as f:
            f.write(html_content)
    
    def generate_csv_report(self, summary: Dict[str, Any], output_file: str):
        """Generate CSV report from summary data"""
        
        with open(output_file, 'w', newline='') as f:
            writer = csv.writer(f)
            
            # Header
            writer.writerow(['Endpoint', 'Total Requests', 'Failed Requests', 'Error Rate (%)', 
                           'Min RT (ms)', 'Avg RT (ms)', 'P90 RT (ms)', 'P95 RT (ms)', 
                           'P99 RT (ms)', 'Max RT (ms)'])
            
            # Data rows
            for endpoint, stats in summary['endpoints'].items():
                writer.writerow([
                    endpoint,
                    stats['count'],
                    stats['errors'],
                    f"{stats['error_rate']:.2f}",
                    stats['min_rt'],
                    f"{stats['avg_rt']:.0f}",
                    f"{stats['p90_rt']:.0f}",
                    f"{stats['p95_rt']:.0f}",
                    f"{stats['p99_rt']:.0f}",
                    stats['max_rt']
                ])
            
            # Summary row
            writer.writerow([])
            writer.writerow(['TOTAL', summary['total_requests'], summary['failed_requests'], 
                           f"{summary['error_rate']:.2f}"])
    
    def generate_all_reports(self, jtl_file: str, tenant: str, test_name: str):
        """Generate all report formats"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Generate summary
        summary = self.generate_summary_report(jtl_file, tenant)
        
        # Save summary JSON
        summary_file = os.path.join(self.results_dir, "summary", f"{test_name}_{tenant}_{timestamp}.json")
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Generate HTML report
        html_file = os.path.join(self.results_dir, "html", f"{test_name}_{tenant}_{timestamp}.html")
        self.generate_html_report(summary, html_file)
        
        # Generate CSV report
        csv_file = os.path.join(self.results_dir, "csv", f"{test_name}_{tenant}_{timestamp}.csv")
        self.generate_csv_report(summary, csv_file)
        
        return {
            "summary": summary_file,
            "html": html_file,
            "csv": csv_file
        }
```

### 2.8 Updated CLI Tool (cli.py)

```python
import click
import json
from generators.payload_generators import get_generator
from utils.tenant_manager import TenantManager
from utils.logger import stress_logger
from utils.report_generator import ReportGenerator
from validators.response_validators import ResponseValidator

@click.group()
def cli():
    """YMS Dashboard Stress Testing CLI"""
    pass

@cli.command()
@click.option('--tenant', required=True, help='Tenant name')
@click.option('--endpoint', required=True, help='API endpoint name')
@click.option('--count', default=1, help='Number of payloads to generate')
@click.option('--output-file', help='Output file for payloads')
def generate(tenant, endpoint, count, output_file):
    """Generate test payloads for specified endpoint"""
    try:
        stress_logger.info(f"Generating {count} payloads for tenant: {tenant}, endpoint: {endpoint}")
        
        manager = TenantManager()
        tenant_config = manager.get_tenant_config(tenant)
        
        generator = get_generator(endpoint, tenant_config)
        
        payloads = []
        for i in range(count):
            payload = generator.generate()
            payloads.append(payload)
            
            if not output_file:
                click.echo(json.dumps(payload, indent=2))
        
        if output_file:
            with open(output_file, 'w') as f:
                json.dump(payloads, f, indent=2)
            stress_logger.info(f"Saved {count} payloads to {output_file}")
            
    except Exception as e:
        stress_logger.error(f"Failed to generate payloads: {str(e)}", exc_info=e)
        click.echo(f"Error: {str(e)}", err=True)
        exit(1)

@cli.command()
@click.option('--response-file', required=True, type=click.File('r'), help='Response file to validate')
@click.option('--endpoint', required=True, help='API endpoint name')
def validate(response_file, endpoint):
    """Validate API response"""
    try:
        validator = ResponseValidator()
        
        response_data = json.load(response_file)
        
        valid, results = validator.validate_response(
            endpoint=endpoint,
            response_code=response_data.get('status_code', 200),
            response_body=json.dumps(response_data.get('body', {})),
            response_time=response_data.get('response_time', 0)
        )
        
        click.echo(json.dumps(results, indent=2))
        
        if not valid:
            stress_logger.warning(f"Validation failed for {endpoint}")
            click.echo("Validation FAILED", err=True)
            exit(1)
        else:
            stress_logger.info(f"Validation passed for {endpoint}")
            click.echo("Validation PASSED")
            
    except Exception as e:
        stress_logger.error(f"Validation error: {str(e)}", exc_info=e)
        click.echo(f"Error: {str(e)}", err=True)
        exit(1)

@cli.command()
@click.option('--jtl-file', required=True, help='JMeter JTL results file')
@click.option('--tenant', required=True, help='Tenant name')
@click.option('--test-name', default='stress-test', help='Test name for report')
def report(jtl_file, tenant, test_name):
    """Generate reports from JMeter results"""
    try:
        stress_logger.info(f"Generating reports for {test_name} - {tenant}")
        
        generator = ReportGenerator()
        report_files = generator.generate_all_reports(jtl_file, tenant, test_name)
        
        click.echo("Reports generated successfully:")
        click.echo(f"  - Summary: {report_files['summary']}")
        click.echo(f"  - HTML: {report_files['html']}")
        click.echo(f"  - CSV: {report_files['csv']}")
        
        # Display summary stats
        with open(report_files['summary'], 'r') as f:
            summary = json.load(f)
            click.echo(f"\nTest Summary:")
            click.echo(f"  Total Requests: {summary['total_requests']:,}")
            click.echo(f"  Failed Requests: {summary['failed_requests']:,}")
            click.echo(f"  Error Rate: {summary['error_rate']:.2f}%")
            
    except Exception as e:
        stress_logger.error(f"Report generation failed: {str(e)}", exc_info=e)
        click.echo(f"Error: {str(e)}", err=True)
        exit(1)

if __name__ == '__main__':
    cli()
```

## 3. Configuration Files

### 3.1 Tenant Configuration (tenant_config.yaml)

```yaml
tenants:
  tenant-alpha:
    name: "tenant-alpha"
    rpm_config:
      yard-availability: 100
      trailer-overview: 150
      trailer-exception-summary: 80
      task-workload-summary: 120
      task-attention-summary: 90
      site-occupancy: 110
      shipment-volume-forecast: 70
      dwell-time-summary: 85
      door-breakdown-summary: 95
      detention-summary: 75
      saved-filter-trailer-count: 60
    data_pool:
      facility_ids: [1001, 1002, 1003, 1004, 1005]
      carrier_ids: [2001, 2002, 2003, 2004, 2005, 2006]
      user_ids: [3001, 3002, 3003]
      saved_filter_ids: [4001, 4002, 4003, 4004]
    auth:
      token: "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
    sla:
      response_time_ms: 2000
      error_rate_percent: 1

  tenant-beta:
    name: "tenant-beta"
    rpm_config:
      yard-availability: 80
      trailer-overview: 120
      trailer-exception-summary: 60
      task-workload-summary: 100
      task-attention-summary: 70
      site-occupancy: 90
      shipment-volume-forecast: 50
      dwell-time-summary: 65
      door-breakdown-summary: 75
      detention-summary: 55
      saved-filter-trailer-count: 45
    data_pool:
      facility_ids: [2001, 2002, 2003]
      carrier_ids: [3001, 3002, 3003, 3004]
      user_ids: [4001, 4002]
      saved_filter_ids: [5001, 5002, 5003]
    auth:
      token: "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
    sla:
      response_time_ms: 2500
      error_rate_percent: 2

  tenant-gamma:
    name: "tenant-gamma"
    rpm_config:
      yard-availability: 120
      trailer-overview: 180
      trailer-exception-summary: 100
      task-workload-summary: 140
      task-attention-summary: 110
      site-occupancy: 130
      shipment-volume-forecast: 90
      dwell-time-summary: 105
      door-breakdown-summary: 115
      detention-summary: 95
      saved-filter-trailer-count: 75
    data_pool:
      facility_ids: [3001, 3002, 3003, 3004]
      carrier_ids: [4001, 4002, 4003, 4004, 4005]
      user_ids: [5001, 5002, 5003, 5004]
      saved_filter_ids: [6001, 6002, 6003, 6004, 6005]
    auth:
      token: "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
    sla:
      response_time_ms: 1500
      error_rate_percent: 0.5
```

### 3.2 Test Scenarios Configuration (test_scenarios.yaml)

```yaml
scenarios:
  smoke_test:
    name: "Smoke Test"
    description: "Quick validation of all endpoints"
    duration_minutes: 5
    ramp_up_seconds: 30
    tenants: ["tenant-alpha"]
    load_multiplier: 0.1  # 10% of configured RPM

  load_test:
    name: "Load Test"
    description: "Standard load test at expected capacity"
    duration_minutes: 60
    ramp_up_seconds: 300
    tenants: ["tenant-alpha", "tenant-beta", "tenant-gamma"]
    load_multiplier: 1.0  # 100% of configured RPM

  stress_test:
    name: "Stress Test"
    description: "Push system beyond normal capacity"
    duration_minutes: 30
    ramp_up_seconds: 180
    tenants: ["tenant-alpha", "tenant-beta", "tenant-gamma"]
    load_multiplier: 1.5  # 150% of configured RPM

  endurance_test:
    name: "Endurance Test"
    description: "Extended duration test for memory leaks"
    duration_minutes: 240
    ramp_up_seconds: 600
    tenants: ["tenant-alpha", "tenant-beta"]
    load_multiplier: 0.8  # 80% of configured RPM

  spike_test:
    name: "Spike Test"
    description: "Sudden load increase simulation"
    duration_minutes: 20
    ramp_up_seconds: 5
    tenants: ["tenant-alpha", "tenant-beta", "tenant-gamma"]
    load_multiplier: 2.0  # 200% of configured RPM
```

## 4. JMeter Integration Script

### 4.1 Groovy Script for Payload Generation (jmeter_integration.groovy)

```groovy
import javax.script.ScriptEngine
import javax.script.ScriptEngineManager
import java.nio.file.Paths

// Get parameters from JMeter
String tenant = vars.get("tenant")
String endpoint = vars.get("endpoint")
String pythonPath = props.get("python.path", "/usr/bin/python3")
String frameworkPath = props.get("framework.path", "../python")

// Build command
String[] command = [
    pythonPath,
    "-m", "cli",
    "generate",
    "--tenant", tenant,
    "--endpoint", endpoint,
    "--count", "1"
]

// Execute Python script
ProcessBuilder pb = new ProcessBuilder(command)
pb.directory(new File(frameworkPath))
Process process = pb.start()

// Read output
BufferedReader reader = new BufferedReader(new InputStreamReader(process.getInputStream()))
StringBuilder output = new StringBuilder()
String line
while ((line = reader.readLine()) != null) {
    output.append(line)
}

// Wait for completion
int exitCode = process.waitFor()

if (exitCode != 0) {
    log.error("Failed to generate payload: " + output.toString())
    throw new Exception("Payload generation failed")
}

// Set the payload in JMeter variable
vars.put("payload", output.toString())
log.info("Generated payload for " + endpoint + ": " + output.toString())
```

### 4.2 Groovy Script for Response Validation (jmeter_validation.groovy)

```groovy
import groovy.json.JsonSlurper
import groovy.json.JsonBuilder

// Get response data from JMeter
String responseData = prev.getResponseDataAsString()
int responseCode = prev.getResponseCode() as Integer
long responseTime = prev.getTime()
String endpoint = vars.get("endpoint")

// Prepare validation data
def validationData = [
    status_code: responseCode,
    body: new JsonSlurper().parseText(responseData),
    response_time: responseTime
]

// Convert to JSON
String jsonData = new JsonBuilder(validationData).toString()

// Execute Python validator
String pythonPath = props.get("python.path", "/usr/bin/python3")
String frameworkPath = props.get("framework.path", "../python")

// Create temporary file with response data
File tempFile = File.createTempFile("response_", ".json")
tempFile.write(jsonData)

String[] command = [
    pythonPath,
    "-m", "cli",
    "validate",
    "--response-file", tempFile.absolutePath,
    "--endpoint", endpoint
]

ProcessBuilder pb = new ProcessBuilder(command)
pb.directory(new File(frameworkPath))
Process process = pb.start()

// Read output
BufferedReader reader = new BufferedReader(new InputStreamReader(process.getInputStream()))
StringBuilder output = new StringBuilder()
String line
while ((line = reader.readLine()) != null) {
    output.append(line)
}

// Wait for completion
int exitCode = process.waitFor()

// Clean up temp file
tempFile.delete()

// Check validation result
if (exitCode != 0) {
    AssertionResult.setFailure(true)
    AssertionResult.setFailureMessage("Validation failed: " + output.toString())
} else {
    log.info("Validation passed for " + endpoint)
}
```

## 5. Execution Scripts

### 5.1 Local Execution Script (run-local.sh)

```bash
#!/bin/bash

# Set variables
JMETER_HOME="${JMETER_HOME:-/opt/jmeter}"
TEST_PLAN="jmeter/test-plans/YMS-Dashboard-Master-Test.jmx"
RESULTS_DIR="results/$(date +%Y%m%d_%H%M%S)"
LOGS_DIR="logs/jmeter"
PYTHON_PATH="$(which python3)"
FRAMEWORK_PATH="$(pwd)/python"

# Create directories
mkdir -p "$RESULTS_DIR"
mkdir -p "$LOGS_DIR"

# Parse command line arguments
TENANT=""
SCENARIO="load_test"
DURATION="300"

while [[ $# -gt 0 ]]; do
    case $1 in
        --tenant)
            TENANT="$2"
            shift 2
            ;;
        --scenario)
            SCENARIO="$2"
            shift 2
            ;;
        --duration)
            DURATION="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Validate tenant
if [ -z "$TENANT" ]; then
    echo "Error: --tenant is required"
    exit 1
fi

# Run JMeter test
echo "Starting stress test for tenant: $TENANT"
echo "Scenario: $SCENARIO"
echo "Duration: $DURATION seconds"
echo "Results will be saved to: $RESULTS_DIR"

# Run JMeter
$JMETER_HOME/bin/jmeter -n \
    -t "$TEST_PLAN" \
    -Jtenant="$TENANT" \
    -Jscenario="$SCENARIO" \
    -Jduration="$DURATION" \
    -Jpython.path="$PYTHON_PATH" \
    -Jframework.path="$FRAMEWORK_PATH" \
    -l "$RESULTS_DIR/results.jtl" \
    -j "$LOGS_DIR/jmeter_${TENANT}_$(date +%Y%m%d_%H%M%S).log" \
    -e -o "$RESULTS_DIR/html-report"

# Generate custom reports
echo "Generating custom reports..."
cd python
python -m cli report \
    --jtl-file "../$RESULTS_DIR/results.jtl" \
    --tenant "$TENANT" \
    --test-name "$SCENARIO"

echo "Test completed. Results available in: $RESULTS_DIR"
echo "View HTML report: $RESULTS_DIR/html-report/index.html"
echo "View custom report: reports/html/"
```

### 5.2 Distributed Execution Script (run-distributed.sh)

```bash
#!/bin/bash

# Set variables
JMETER_HOME="${JMETER_HOME:-/opt/jmeter}"
TEST_PLAN="jmeter/test-plans/YMS-Dashboard-Master-Test.jmx"
RESULTS_DIR="results/distributed_$(date +%Y%m%d_%H%M%S)"
LOGS_DIR="logs/jmeter"
PYTHON_PATH="$(which python3)"
FRAMEWORK_PATH="$(pwd)/python"

# JMeter distributed settings
REMOTE_HOSTS="${REMOTE_HOSTS:-localhost:1099,localhost:2099}"

# Create directories
mkdir -p "$RESULTS_DIR"
mkdir -p "$LOGS_DIR"

# Parse command line arguments
TENANTS=""
SCENARIO="load_test"

while [[ $# -gt 0 ]]; do
    case $1 in
        --tenants)
            TENANTS="$2"
            shift 2
            ;;
        --scenario)
            SCENARIO="$2"
            shift 2
            ;;
        --remote-hosts)
            REMOTE_HOSTS="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            exit 1
            ;;
    esac
done

# Start distributed test
echo "Starting distributed stress test"
echo "Tenants: $TENANTS"
echo "Scenario: $SCENARIO"
echo "Remote hosts: $REMOTE_HOSTS"

# Run distributed test
$JMETER_HOME/bin/jmeter -n \
    -t "$TEST_PLAN" \
    -R "$REMOTE_HOSTS" \
    -Gtenants="$TENANTS" \
    -Gscenario="$SCENARIO" \
    -Gpython.path="$PYTHON_PATH" \
    -Gframework.path="$FRAMEWORK_PATH" \
    -l "$RESULTS_DIR/results.jtl" \
    -j "$LOGS_DIR/jmeter_distributed_$(date +%Y%m%d_%H%M%S).log" \
    -e -o "$RESULTS_DIR/html-report"

# Generate reports for each tenant
IFS=',' read -ra TENANT_ARRAY <<< "$TENANTS"
for tenant in "${TENANT_ARRAY[@]}"; do
    echo "Generating report for tenant: $tenant"
    cd python
    python -m cli report \
        --jtl-file "../$RESULTS_DIR/results.jtl" \
        --tenant "$tenant" \
        --test-name "$SCENARIO"
    cd ..
done

echo "Test completed. Results available in: $RESULTS_DIR"
```

### 5.3 Report Analysis Script (analyze-results.py)

```python
#!/usr/bin/env python3

import sys
import json
import csv
from datetime import datetime
from pathlib import Path

def analyze_results(results_dir):
    """Analyze all test results and generate comparison report"""
    
    results_path = Path(results_dir)
    summary_files = list(results_path.glob("summary/*.json"))
    
    if not summary_files:
        print("No summary files found")
        return
    
    # Load all summaries
    summaries = []
    for file in summary_files:
        with open(file, 'r') as f:
            summaries.append(json.load(f))
    
    # Sort by date
    summaries.sort(key=lambda x: x['test_date'])
    
    # Generate comparison report
    print("\n=== YMS Dashboard Stress Test Analysis ===\n")
    print(f"Total tests analyzed: {len(summaries)}")
    print(f"Date range: {summaries[0]['test_date']} to {summaries[-1]['test_date']}\n")
    
    # Tenant summary
    tenants = {}
    for summary in summaries:
        tenant = summary['tenant']
        if tenant not in tenants:
            tenants[tenant] = {
                'tests': 0,
                'total_requests': 0,
                'total_errors': 0,
                'avg_error_rate': []
            }
        
        tenants[tenant]['tests'] += 1
        tenants[tenant]['total_requests'] += summary['total_requests']
        tenants[tenant]['total_errors'] += summary['failed_requests']
        tenants[tenant]['avg_error_rate'].append(summary['error_rate'])
    
    print("=== Tenant Summary ===")
    for tenant, stats in tenants.items():
        avg_error = sum(stats['avg_error_rate']) / len(stats['avg_error_rate'])
        print(f"\n{tenant}:")
        print(f"  Tests run: {stats['tests']}")
        print(f"  Total requests: {stats['total_requests']:,}")
        print(f"  Total errors: {stats['total_errors']:,}")
        print(f"  Average error rate: {avg_error:.2f}%")
    
    # Endpoint performance trends
    print("\n=== Endpoint Performance Trends ===")
    endpoint_stats = {}
    
    for summary in summaries:
        for endpoint, stats in summary['endpoints'].items():
            if endpoint not in endpoint_stats:
                endpoint_stats[endpoint] = {
                    'p95_times': [],
                    'error_rates': []
                }
            endpoint_stats[endpoint]['p95_times'].append(stats['p95_rt'])
            endpoint_stats[endpoint]['error_rates'].append(stats['error_rate'])
    
    for endpoint, stats in endpoint_stats.items():
        avg_p95 = sum(stats['p95_times']) / len(stats['p95_times'])
        avg_error = sum(stats['error_rates']) / len(stats['error_rates'])
        print(f"\n{endpoint}:")
        print(f"  Average P95 response time: {avg_p95:.0f}ms")
        print(f"  Average error rate: {avg_error:.2f}%")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python analyze-results.py <results_directory>")
        sys.exit(1)
    
    analyze_results(sys.argv[1])
```